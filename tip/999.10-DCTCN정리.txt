깃허브주소
https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks


* 트랜스포머 기반 립리딩 논문 들 
1. Tokenized Lip Reading (2020)
발표 시점: 2020년
구조: 트랜스포머와 ResNet을 결합한 모델로, 트랜스포머의 멀티 헤드 어텐션 메커니즘을 사용하여 입술 모양에서 텍스트를 생성합니다. ResNet은 이미지의 시각적 특징을 추출하고, 트랜스포머는 이를 시퀀스로 처리하여 문장 단위로 예측합니다.
정확도: LRW 데이터셋 기준으로 **88.75%**의 정확도를 기록했습니다​
연산량: 트랜스포머 기반이므로 일반적인 CNN보다 연산량이 높지만, ResNet을 결합하여 효율성을 개선했습니다.

2. Mini-3DCvT (2022)
발표 시점: 2022년
구조: 3D 컨볼루션과 비전 트랜스포머(3D Conv + Vision Transformer)를 결합한 구조로, 입술 모양의 시공간적 패턴을 분석하여 텍스트를 예측합니다. 3D Conv는 공간적 특징을, 트랜스포머는 시간적 특징을 포착합니다.
정확도: **88.5%**의 정확도를 달성했습니다​(
연산량: 전통적인 트랜스포머와 비교해 효율성을 개선했지만, 여전히 상당한 연산 자원을 필요로 합니다. 다만 기존 TCN 기반 모델들보다 연산 효율성이 우수한 편입니다.

3. EfficientNetv2 + Transformer (2022)
발표 시점: 2022년
구조: EfficientNetv2와 트랜스포머를 결합한 모델로, EfficientNetv2는 CNN을 통해 입술 이미지의 시각적 특징을 추출하고, 트랜스포머가 시간적 정보를 처리하여 텍스트로 변환합니다. 이 모델은 경량화와 효율성을 중점적으로 설계되었습니다.
정확도: **89.52%**로, LRW 데이터셋에서 최고 성능을 기록했습니다​
연산량: 기존의 트랜스포머 기반 모델보다 5배 적은 연산량으로 실시간 처리에 적합하며, 경량화에도 불구하고 높은 정확도를 유지합니다.




* 지피티 추천 아키텍처

1. EfficientNetv2 대체: ConvNeXt 또는 EfficientNet-L
ConvNeXt는 ResNet을 개선한 모델로, 최신 CNN 기술을 활용하여 더 높은 성능을 유지하면서도 연산 효율성을 극대화합니다. ConvNeXt는 단순한 디자인과 향상된 CNN 블록 덕분에 EfficientNet보다 더 빠르고 정확한 시각적 특징 추출이 가능합니다.
효과: 시각적 특징 추출을 더 효율적으로 수행하여, 입술 모양 인식의 초기 처리 성능을 크게 높일 수 있습니다.

2. Transformer 대체: Swin Transformer 또는 Perceiver
Swin Transformer는 기존 트랜스포머의 연산량을 줄이면서도, 이미지 처리에서 뛰어난 성능을 보입니다. Swin Transformer는 계층적 구조를 통해 트랜스포머의 글로벌 어텐션 문제를 해결하고, 이미지의 지역적 정보를 더욱 효율적으로 처리합니다. 이는 특히 립리딩처럼 세밀한 영상 분석에 적합합니다.
Perceiver는 대규모 입력 데이터를 효율적으로 처리할 수 있도록 설계된 트랜스포머 변형 모델입니다. 이 모델은 연산량을 줄이면서도 멀티모달 데이터를 효과적으로 처리할 수 있어, 립리딩과 같은 비디오 기반 작업에 매우 적합합니다.

3. 멀티모달 학습 적용: Wav2Vec2 + Swin Transformer
립리딩 성능을 극대화하기 위해 음성 정보를 결합할 수 있습니다. Wav2Vec2는 음성 신호에서 텍스트를 추출하는 데 매우 강력한 성능을 보이는 모델로, Swin Transformer와 결합하여 비디오와 오디오 데이터를 동시에 처리하는 멀티모달 학습을 적용할 수 있습니다.
효과: 비디오와 오디오 정보를 결합함으로써, 잡음이 있는 환경이나 제한된 시각적 정보만 있는 경우에도 더 높은 성능을 기대할 수 있습니다.

4. 전처리 개선: Patch Embedding 및 ROI 확대
Patch Embedding 기법을 통해 이미지를 작은 패치로 나눈 후 각 패치에 대해 독립적인 학습을 수행할 수 있습니다. 이는 입술 모양뿐 아니라 주변 얼굴 정보까지 함께 고려할 수 있어, 전반적인 인식 정확도를 높일 수 있습니다.
ROI(Region of Interest) 크기를 확대하여 입술 주변의 추가적인 비언어적 단서(예: 표정 변화, 턱 움직임)를 학습할 수 있습니다.

5. 학습 방법 최적화: Contrastive Learning과 Self-Supervised Learning
대조 학습(Contrastive Learning) 기법을 적용하여, 서로 다른 영상 간의 특징 차이를 학습하도록 하면 더욱 강력한 특징을 학습할 수 있습니다. 특히 립리딩에서는 비슷한 입술 모양의 미세한 차이를 학습하는 데 도움이 될 수 있습니다.
Self-Supervised Learning은 데이터 레이블이 부족한 상황에서도 성능을 유지할 수 있게 해줍니다. unlabeled 데이터를 효과적으로 사용할 수 있으며, 립리딩 모델의 일반화 성능을 향상시킬 수 있습니다.

종합 아키텍처 제안:
전면부(Frontend): ConvNeXt 또는 EfficientNet-L으로 고해상도 시각적 특징 추출
백엔드(Backbone): Swin Transformer 또는 Perceiver를 사용하여 더 높은 정확도와 효율성을 제공하는 시간적 패턴 학습
멀티모달 결합: Wav2Vec2를 추가해 음성 신호와 비디오 신호를 함께 처리
학습 최적화: Contrastive Learning 및 Self-Supervised Learning을 통해 비디오 데이터 간의 미세한 차이를 효과적으로 학습
이와 같은 아키텍처는 기존 EfficientNetv2 + Transformer 모델보다 더 높은 정확도와 연산 효율성을 달성할 수 있으며, 특히 실시간 립리딩 및 복잡한 비디오 데이터 처리에서 큰 개선을 기대할 수 있습니다.



* 위는 이미지 기반. 지금부터는 영상 기반
스윈 트랜스포머 논문 리뷰
https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows

convNext 논문 리뷰
https://velog.io/@pre_f_86/ConvNeXt-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0



