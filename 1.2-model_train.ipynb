{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1675b9-eb48-4131-a80f-910ad1259592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision timm transformers opencv-python\n",
    "# !apt-get install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6c1177-ba7e-4b36-b92f-078ca91b458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9d3575-c5f0-4223-b837-5b7cf8671229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, frame_paths, annotations, tokenizer, transform, fps, frames_per_chunk=30, load_from_file=None):\n",
    "        self.frame_paths = frame_paths  # 비디오 프레임 경로 리스트의 리스트\n",
    "        self.annotations = annotations  # 어노테이션 리스트의 리스트\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.fps = fps  # 프레임 레이트\n",
    "        self.frames_per_chunk = frames_per_chunk  # 시퀀스 당 프레임 수\n",
    "\n",
    "        if load_from_file is not None:\n",
    "            self.load_dataset(load_from_file)\n",
    "        else:\n",
    "            self.data = self.prepare_dataset()\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        data_list = []\n",
    "        for video_frames, video_annotations in zip(self.frame_paths, self.annotations):\n",
    "            num_frames = len(video_frames)\n",
    "            frame_text_mapping = self.map_frames_to_text(video_frames, video_annotations, self.fps, num_frames, self.frames_per_chunk)\n",
    "            for start_frame, end_frame, text in frame_text_mapping:\n",
    "                # 프레임 로드 및 전처리\n",
    "                frames = []\n",
    "                for frame_idx in range(start_frame, end_frame + 1):\n",
    "                    if frame_idx >= len(video_frames):\n",
    "                        break  # 인덱스 오류 방지\n",
    "                    frame_path = video_frames[frame_idx]\n",
    "                    frame = self.preprocess_video_frame(frame_path)\n",
    "                    frames.append(frame)\n",
    "                # 필요한 경우 프레임 패딩\n",
    "                while len(frames) < self.frames_per_chunk:\n",
    "                    frames.append(torch.zeros_like(frames[0]))\n",
    "                frames_tensor = torch.stack(frames)  # (frames_per_chunk, C, H, W)\n",
    "                # 텍스트 토크나이즈\n",
    "                tokens = self.tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=50,\n",
    "                    truncation=True\n",
    "                )\n",
    "                input_ids = tokens['input_ids'].squeeze(0)  # (max_length,)\n",
    "                attention_mask = tokens['attention_mask'].squeeze(0)  # (max_length,)\n",
    "                data_list.append({\n",
    "                    'video': frames_tensor,\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                })\n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 총 시퀀스 수\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def preprocess_video_frame(self, frame_path):\n",
    "        img = Image.open(frame_path).convert('RGB')\n",
    "        img_tensor = self.transform(img)\n",
    "        return img_tensor\n",
    "\n",
    "    def map_frames_to_text(self, frame_paths, annotations, fps, num_frames, frames_per_chunk):\n",
    "        frame_text_mapping = []\n",
    "        for start_time, end_time, text in annotations:\n",
    "            total_text_frames = int((end_time - start_time) * fps)\n",
    "            if total_text_frames == 0:\n",
    "                continue\n",
    "            words = text.split()\n",
    "            total_words = len(words)\n",
    "            frames_per_word = [max(1, total_text_frames // total_words)] * total_words\n",
    "            leftover_frames = total_text_frames - sum(frames_per_word)\n",
    "            for i in range(leftover_frames):\n",
    "                frames_per_word[i % total_words] += 1\n",
    "            current_frame = int(start_time * fps)\n",
    "            for word, frame_count in zip(words, frames_per_word):\n",
    "                start_frame = current_frame\n",
    "                end_frame = min(current_frame + frame_count - 1, num_frames - 1)\n",
    "                frame_text_mapping.append((start_frame, end_frame, word))\n",
    "                current_frame += frame_count\n",
    "        # 시퀀스 길이에 맞게 프레임을 묶음\n",
    "        sequences = []\n",
    "        for i in range(0, len(frame_text_mapping), frames_per_chunk):\n",
    "            seq_frames = frame_text_mapping[i:i+frames_per_chunk]\n",
    "            if len(seq_frames) == 0:\n",
    "                continue\n",
    "            start_frame = seq_frames[0][0]\n",
    "            end_frame = seq_frames[-1][1]\n",
    "            texts = ' '.join([t[2] for t in seq_frames])\n",
    "            sequences.append((start_frame, end_frame, texts))\n",
    "        return sequences\n",
    "\n",
    "    # 데이터셋 저장 함수\n",
    "    def save_dataset(self, file_path):\n",
    "        # 데이터를 저장\n",
    "        torch.save(self.data, file_path)\n",
    "        print(f\"Dataset saved to {file_path}\")\n",
    "    \n",
    "    # 데이터셋 로드 함수\n",
    "    def load_dataset(self, file_path):\n",
    "        self.data = torch.load(file_path)\n",
    "        print(f\"Dataset loaded from {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b832472-8613-469d-baa5-92ad42db68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frontend(nn.Module):\n",
    "    def __init__(self, model_type=\"convnext\"):\n",
    "        super(Frontend, self).__init__()\n",
    "        if model_type == \"convnext\":\n",
    "            self.model = timm.create_model('convnext_base', pretrained=True)\n",
    "            feature_dim = self.model.get_classifier().in_features\n",
    "            self.model.reset_classifier(0)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x  # (batch_size * seq_len, feature_dim)\n",
    "\n",
    "class LipReadingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim, frontend_type=\"convnext\"):\n",
    "        super(LipReadingModel, self).__init__()\n",
    "        self.frontend = Frontend(model_type=frontend_type)\n",
    "        self.lstm = nn.LSTM(input_size=feature_dim, hidden_size=512, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, video):\n",
    "        batch_size, seq_len, C, H, W = video.size()\n",
    "        video = video.view(batch_size * seq_len, C, H, W)\n",
    "        features = self.frontend(video)  # (batch_size * seq_len, feature_dim)\n",
    "        features = features.view(batch_size, seq_len, -1)  # (batch_size, seq_len, feature_dim)\n",
    "        outputs, _ = self.lstm(features)  # (batch_size, seq_len, hidden_size)\n",
    "        outputs = self.fc(outputs)  # (batch_size, seq_len, vocab_size)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54b503-c9c3-4a9c-be3a-31eb2ba1be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SavedLipReadingDataset(Dataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        # 영상 프레임 경로를 사용하여 이미지를 로드하고 transform 적용\n",
    "        video_frames = data['video_paths']  # 이미지 경로 리스트\n",
    "        frames = [self.transform(Image.open(frame).convert('RGB')) for frame in video_frames]\n",
    "        video_tensor = torch.stack(frames)\n",
    "        return {\n",
    "            'video': video_tensor,\n",
    "            'input_ids': data['input_ids'],\n",
    "            'attention_mask': data['attention_mask']\n",
    "        }\n",
    "\n",
    "# 데이터셋 로드 함수\n",
    "def load_dataset(file_path):\n",
    "    # 저장된 데이터를 불러오기\n",
    "    data_list = torch.load(file_path)\n",
    "    print(f\"Dataset loaded from {file_path}\")\n",
    "    return data_list\n",
    "\n",
    "# 토크나이저 설정\n",
    "tokenizer = BertTokenizer.from_pretrained(\"beomi/kobert\")\n",
    "\n",
    "# 저장된 데이터셋 불러오기\n",
    "save_path = 'lip_reading_dataset.pt'\n",
    "loaded_data = load_dataset(save_path)\n",
    "dataset = SavedLipReadingDataset(loaded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a233b87-9cc8-46a0-bf91-e35842416fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 추론 시에 필요한 변환기 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 추론 함수\n",
    "def infer(model, video_frames, tokenizer, transform, device='cpu'):\n",
    "    model.eval()\n",
    "    processed_frames = [transform(Image.open(frame).convert('RGB')) for frame in video_frames]\n",
    "    video_tensor = torch.stack(processed_frames).unsqueeze(0).to(device)  # (1, seq_len, C, H, W)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(video_tensor)  # (1, seq_len, vocab_size)\n",
    "        predicted_ids = torch.argmax(outputs, dim=-1).squeeze(0)  # (seq_len,)\n",
    "    predicted_text = tokenizer.decode(predicted_ids.tolist(), skip_special_tokens=True)\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78887f2e-373e-499e-acef-958b5356c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수, 최적화 설정\n",
    "vocab_size = tokenizer.vocab_size\n",
    "feature_dim = 1024  # convnext_base의 출력 차원\n",
    "model = LipReadingModel(vocab_size=vocab_size, feature_dim=feature_dim, frontend_type=\"convnext\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 모델 학습 시작\n",
    "train_model(model, dataloader, criterion, optimizer, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e4366-c3da-4dfa-b651-168c1fcbb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 후 DataLoader 생성\n",
    "# 저장된 데이터셋 불러오기\n",
    "save_path = 'lip_reading_dataset.pt'\n",
    "loaded_data = load_dataset(save_path)\n",
    "\n",
    "# 불러온 데이터 확인\n",
    "for i in range(3):  # 첫 3개 항목만 확인\n",
    "    print(f\"Video Shape: {loaded_data[i]['video'].shape}, Input IDs: {loaded_data[i]['input_ids']}, Attention Mask: {loaded_data[i]['attention_mask']}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델, 손실 함수, 최적화 설정\n",
    "model = LipReadingModel(frontend_type=\"convnext\", backend_type=\"swin\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 시작\n",
    "train_model(model, dataloader, criterion, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde470a-2da0-4f1e-b563-8ef8f2284830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925a615-bc31-488f-8fca-dd2f522a743c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63534bcb-70f2-4082-9016-d992bf0f62dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58d398-a231-4526-9ae7-3a75f42124de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df3378-4c13-447e-8b59-ca0621057a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12822b5-d579-44b9-a2a1-30a3cf3d40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, video_frames, tokenizer, transform, device='cpu'):\n",
    "    model.eval()\n",
    "    processed_frames = [transform(Image.open(frame).convert('RGB')) for frame in video_frames]\n",
    "    video_tensor = torch.stack(processed_frames).unsqueeze(0).to(device)  # (1, seq_len, C, H, W)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(video_tensor)  # (1, seq_len, vocab_size)\n",
    "        predicted_ids = torch.argmax(outputs, dim=-1).squeeze(0)  # (seq_len,)\n",
    "    predicted_text = tokenizer.decode(predicted_ids.tolist(), skip_special_tokens=True)\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e9c43-1b66-4daf-b37f-d2f696c9a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"beomi/kobert\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = LipReadingDataset(\n",
    "    frame_paths=[list_of_frame_paths],  # 비디오 프레임 경로 리스트의 리스트\n",
    "    annotations=[list_of_annotations],  # 어노테이션 리스트의 리스트\n",
    "    tokenizer=tokenizer,\n",
    "    transform=transform,\n",
    "    fps=25,\n",
    "    frames_per_chunk=30\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "# 테스트할 비디오 디렉토리 경로 예시\n",
    "test_video_dir = './videos/video1'  # 예시 비디오 디렉토리\n",
    "video_frames = load_test_video_frames(test_video_dir)  # 테스트 비디오 프레임 경로 리스트\n",
    "\n",
    "model = LipReadingModel(frontend_type=\"convnext\", backend_type=\"swin\").to(device)  # 학습된 모델을 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"beomi/kobert\")  # 텍스트 추출용 BERT 토크나이저\n",
    "\n",
    "predicted_text = infer(model, video_frames, tokenizer, transform, device)\n",
    "print(\"Predicted Text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cd0b4-47e5-4312-b114-185faa34a4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf45f0c-1e5e-4ece-8be8-45b260a59cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b934e9-5896-4e1c-aaea-2407d3f62da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
