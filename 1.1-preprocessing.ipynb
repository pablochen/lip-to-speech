{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776233b4-8d4a-4993-bf41-d99eee9abbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fc75d9-3662-4745-8389-d282d7b3dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'a' 디렉토리 경로 설정\n",
    "# a_directory = \"/root/lip-to-speech/TL163\"\n",
    "\n",
    "# # 'a' 디렉토리 하위의 첫 번째 디렉토리로 들어가서 그 내부의 모든 파일을 'a' 디렉토리로 이동\n",
    "# current_path = a_directory\n",
    "\n",
    "# # 1단계: 첫 번째 디렉토리로 이동\n",
    "# subdirs = [d for d in os.listdir(current_path) if os.path.isdir(os.path.join(current_path, d))]\n",
    "# if not subdirs:\n",
    "#     raise Exception(\"첫 번째 디렉토리를 찾을 수 없습니다.\")\n",
    "# first_subdir = subdirs[0]\n",
    "# current_path = os.path.join(current_path, first_subdir)\n",
    "\n",
    "# # 2단계: 계속해서 하위 디렉토리로 이동 (가장 안쪽 디렉토리를 찾기)\n",
    "# while True:\n",
    "#     subdirs = [d for d in os.listdir(current_path) if os.path.isdir(os.path.join(current_path, d))]\n",
    "#     if not subdirs:\n",
    "#         break  # 더 이상 하위 디렉토리가 없으면 중단\n",
    "#     current_path = os.path.join(current_path, subdirs[0])\n",
    "\n",
    "# # 3단계: 가장 안쪽 디렉토리 안의 모든 파일을 'a' 디렉토리로 이동\n",
    "# for file_name in os.listdir(current_path):\n",
    "#     file_path = os.path.join(current_path, file_name)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         shutil.move(file_path, a_directory)\n",
    "\n",
    "# # 4단계: 중간 디렉토리들 모두 삭제 (첫 번째 디렉토리부터 시작해서 가장 안쪽까지 삭제)\n",
    "# shutil.rmtree(os.path.join(a_directory, first_subdir))\n",
    "\n",
    "# print(f\"모든 파일을 '{a_directory}'로 이동하고 중간 디렉토리를 삭제했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f7901b-f748-418c-9e5f-d8567ad7fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(json_file_path, video_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_name = data[0]['Video_info']['video_Name']\n",
    "    video_path = f'{video_file_path}/{video_name}'\n",
    "    \n",
    "    FPS = int(data[0]['Audio_info']['Sampling_rate'].replace('Khz',''))\n",
    "    Sentence_info = data[0]['Sentence_info']\n",
    "    \n",
    "    annotations = []\n",
    "    for sentence in Sentence_info:\n",
    "        annotations.append( (sentence['start_time'], sentence['end_time'], sentence['sentence_text']) )\n",
    "\n",
    "    return video_name, FPS, annotations\n",
    "\n",
    "# 3. 전처리 함수 정의\n",
    "def preprocess_video_frame(frame_path, transform):\n",
    "    img = Image.open(frame_path).convert('RGB')\n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor\n",
    "\n",
    "# 비디오 파일 경로 및 저장할 디렉토리 설정\n",
    "def save_frame(video_path, transform):\n",
    "    output_dir = video_path.replace('/TS', '/split_TS')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # 총 프레임 수 확인\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"총 프레임 수: {total_frames}\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break  # 비디오가 끝나면 종료\n",
    "        \n",
    "        print(f\"처리 중인 프레임: {frame_idx + 1}/{total_frames}\")\n",
    "        \n",
    "        # OpenCV에서 BGR로 읽어오기 때문에 RGB로 변환\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # NumPy 배열을 PIL 이미지로 변환\n",
    "        pil_image = Image.fromarray(frame)\n",
    "        \n",
    "        # 변환 적용\n",
    "        transformed_frame = transform(pil_image)\n",
    "    \n",
    "        # 프레임 저장 (원본 이미지로 저장할 경우)\n",
    "        frame_filename = os.path.join(output_dir, f'frame_{frame_idx:04d}.jpg')\n",
    "        pil_image.save(frame_filename)\n",
    "    \n",
    "        # 텐서로 변환된 프레임을 저장하거나 활용하고 싶다면,\n",
    "        # torch.save(transformed_frame, 'path_to_save_tensor')\n",
    "    \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"저장된 프레임 수: {frame_idx}\")\n",
    "\n",
    "    return output_dir\n",
    "    \n",
    "def tokenize_text(text, tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=50)\n",
    "    return tokens\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "        break\n",
    "    \n",
    "    return file_paths\n",
    "    \n",
    "def process_video(json_file_name):\n",
    "    cur_file_path = f'{json_file_path}/{json_file_name}'\n",
    "    video_name, FPS, annotations = get_video_info(cur_file_path, video_file_path)\n",
    "\n",
    "    video_path = f'{video_file_path}/{video_name}'\n",
    "    print(f'{video_name} is started')\n",
    "    output_dir = save_frame(video_path, transform)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1873cc-8242-42f6-9ac4-14ff18cd31b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# json_file_path = '/root/lip-to-speech/TL163'\n",
    "# video_file_path = json_file_path.replace('/TL', '/TS')\n",
    "\n",
    "# json_file_names = os.listdir(json_file_path)\n",
    "\n",
    "# # 멀티스레딩으로 비디오 처리\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = list(executor.map(process_video, json_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1efc509-e5d2-479f-9e55-fa9b05a9f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_file_path = '/root/lip-to-speech/TL164'\n",
    "# video_file_path = json_file_path.replace('/TL', '/TS')\n",
    "\n",
    "# json_file_names = os.listdir(json_file_path)\n",
    "\n",
    "# # 멀티스레딩으로 비디오 처리\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = list(executor.map(process_video, json_file_names))\n",
    "\n",
    "\n",
    "# json_file_path = '/root/lip-to-speech/TL165'\n",
    "# video_file_path = json_file_path.replace('/TL', '/TS')\n",
    "\n",
    "# json_file_names = os.listdir(json_file_path)\n",
    "\n",
    "# # 멀티스레딩으로 비디오 처리\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = list(executor.map(process_video, json_file_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6122afe8-4c3e-4658-b893-025c16ce2f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/lip-to-speech/TL163/lip_K_5_F_06_C651_A_001.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_name = json_file_names[0]\n",
    "tec = f'{json_file_path}/{json_file_name}'\n",
    "tec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebd8c99-f6bc-4f4d-b240-7a72db3fc700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations = []\n",
    "frame_paths = []\n",
    "json_file_path = '/root/lip-to-speech/TL163'\n",
    "\n",
    "json_file_names = os.listdir(json_file_path)\n",
    "for json_file_name in json_file_names:\n",
    "    cur_file_path = f'{json_file_path}/{json_file_name}'\n",
    "    video_file_path = cur_file_path.replace('TL', 'split_TS').replace('.json', '.mp4')\n",
    "    video_name, FPS, annotation = get_video_info(cur_file_path, video_file_path)\n",
    "    annotations.append(annotation)\n",
    "    \n",
    "    cur_frame_path = json_file_path.replace('TL', 'split_TS') + '/' + video_name\n",
    "    frame_list =  os.listdir(cur_frame_path)\n",
    "    cur_frame_list = [ f'{cur_frame_path}/{frame_file}' for frame_file in frame_list ]\n",
    "    frame_paths.append(cur_frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb84b7da-2af4-482b-9104-cd1fab66e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# def map_frames_to_text(video_path, annotations, fps, num_frames):\n",
    "#     frame_text_mapping = {}\n",
    "#     for frame in range(num_frames):\n",
    "#         time_sec = frame / fps  # Convert frame to seconds\n",
    "#         for (start, end, text) in annotations:\n",
    "#             if start <= time_sec < end:\n",
    "#                 frame_text_mapping[frame] = text\n",
    "#                 break\n",
    "#         else:\n",
    "#             frame_text_mapping[frame] = \"\"  # If no matching text, assign empty string\n",
    "#     return frame_text_mapping\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# PyTorch Dataset class to handle 5-frame sequences\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, frame_paths, annotations, fps, frames_per_chunk=5):\n",
    "        self.frame_paths = frame_paths  # Frame paths for video\n",
    "        self.annotations = annotations  # (start, end, text) annotations\n",
    "        self.fps = fps  # Frames per second\n",
    "        self.frames_per_chunk = frames_per_chunk  # Length of frame sequences\n",
    "        \n",
    "        # Prepare the frame-text mapping\n",
    "        num_frames = len(frame_paths[0])  # Number of frames in the video\n",
    "        self.frame_text_mapping = map_frames_to_text(frame_paths[0], annotations[0], fps, num_frames, frames_per_chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_text_mapping)  # Total number of 5-frame sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_frame, end_frame, text = self.frame_text_mapping[idx]\n",
    "        \n",
    "        # Load the frames corresponding to this 5-frame sequence\n",
    "        frames = []\n",
    "        for frame_idx in range(start_frame, end_frame + 1):\n",
    "            frame_path = self.frame_paths[0][frame_idx]  # 실제 프레임 경로를 사용\n",
    "            frame = preprocess_video_frame(frame_path)  # 프레임 로드 및 전처리\n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Combine frames and text\n",
    "        frames_tensor = torch.stack(frames)  # (5, C, H, W)\n",
    "        \n",
    "        return {\n",
    "            'frames': frames_tensor,  # 5프레임 묶음\n",
    "            'text': text  # 5프레임에 대응하는 텍스트\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc1c3c1e-31cc-4f51-9b59-4bbff45c00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frames_to_text(frame_paths, annotations, fps, num_frames, max_sequence_length=10):\n",
    "    frame_text_mapping = []\n",
    "    \n",
    "    for start, end, text in annotations:\n",
    "        # 전체 프레임 중 해당 텍스트가 걸리는 프레임 수\n",
    "        total_text_frames = int((end - start) * fps)\n",
    "        if total_text_frames == 0:\n",
    "            continue\n",
    "\n",
    "        # 텍스트를 띄어쓰기 단위로 분리 (단어 단위)\n",
    "        words = text.split()  # 띄어쓰기 기준으로 텍스트를 나눔\n",
    "        total_words = len(words)\n",
    "\n",
    "        # 각 단어에 할당할 프레임 수 계산 (단어 길이와 비례하게)\n",
    "        # 'max_frame'이라는 새로운 이름을 사용하여 'max'와 충돌 방지\n",
    "        frames_per_word = [max_frame(1, total_text_frames // total_words)] * total_words\n",
    "        leftover_frames = total_text_frames - sum(frames_per_word)\n",
    "\n",
    "        # 나머지 프레임을 고르게 분배\n",
    "        for i in range(leftover_frames):\n",
    "            frames_per_word[i % total_words] += 1\n",
    "\n",
    "        # 유동적인 시퀀스 생성\n",
    "        current_frame = int(start * fps)\n",
    "        for word, frame_count in zip(words, frames_per_word):\n",
    "            # 각 단어에 대해 프레임을 할당\n",
    "            start_frame = current_frame\n",
    "            end_frame = min(current_frame + frame_count - 1, num_frames - 1)\n",
    "            frame_text_mapping.append((start_frame, end_frame, word))\n",
    "            current_frame += frame_count\n",
    "\n",
    "    return frame_text_mapping\n",
    "\n",
    "# 'max_frame' 함수로 'max' 대체\n",
    "def max_frame(a, b):\n",
    "    return a if a > b else b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0b3ce83-ca63-4ce3-ad07-afb99a576743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(117, 131, '나한테'),\n",
       " (132, 145, '맞는'),\n",
       " (146, 159, '화장품을'),\n",
       " (160, 173, '찾기가'),\n",
       " (174, 187, '어려운'),\n",
       " (188, 201, '것'),\n",
       " (202, 215, '같아.'),\n",
       " (249, 264, '다음'),\n",
       " (265, 280, '주에'),\n",
       " (281, 295, '친구'),\n",
       " (296, 310, '생일이어서'),\n",
       " (311, 325, '오늘'),\n",
       " (326, 340, '다른'),\n",
       " (341, 355, '친구들과'),\n",
       " (356, 370, '생일'),\n",
       " (371, 385, '선물을'),\n",
       " (386, 400, '사러'),\n",
       " (401, 415, '갈'),\n",
       " (416, 430, '거야.'),\n",
       " (629, 637, '안'),\n",
       " (638, 646, '넘어가.'),\n",
       " (749, 767, '작년'),\n",
       " (768, 786, '가을에'),\n",
       " (787, 805, '우리'),\n",
       " (806, 824, '아이랑'),\n",
       " (825, 843, '함께'),\n",
       " (844, 862, '공원에서'),\n",
       " (863, 881, '낙엽'),\n",
       " (882, 900, '가지고'),\n",
       " (901, 919, '놀았는데.'),\n",
       " (920, 938, '새벽마다'),\n",
       " (939, 957, '목이'),\n",
       " (958, 976, '말라서'),\n",
       " (977, 995, '잠에서'),\n",
       " (996, 1014, '깨는데'),\n",
       " (1015, 1033, '이제는'),\n",
       " (1034, 1052, '침대'),\n",
       " (1053, 1071, '옆에'),\n",
       " (1072, 1090, '물을'),\n",
       " (1091, 1109, '두고'),\n",
       " (1110, 1127, '자야겠어.'),\n",
       " (1427, 1448, '요즘'),\n",
       " (1449, 1470, '해가'),\n",
       " (1471, 1491, '짧아져서'),\n",
       " (1492, 1512, '퇴근하면'),\n",
       " (1513, 1533, '어둡더라고.'),\n",
       " (1565, 1584, '시간이'),\n",
       " (1585, 1604, '나면'),\n",
       " (1605, 1624, '티브이를'),\n",
       " (1625, 1644, '보면서'),\n",
       " (1645, 1663, '뜨개질을'),\n",
       " (1664, 1682, '해요.'),\n",
       " (1720, 1737, '새로'),\n",
       " (1738, 1754, '산'),\n",
       " (1755, 1771, '니트'),\n",
       " (1772, 1788, '사이즈가'),\n",
       " (1789, 1805, '안'),\n",
       " (1806, 1822, '맞아서'),\n",
       " (1823, 1839, '교환하러'),\n",
       " (1840, 1856, '가야겠어.'),\n",
       " (1930, 1945, '집'),\n",
       " (1946, 1961, '밖에'),\n",
       " (1962, 1977, '나가기'),\n",
       " (1978, 1993, '싫어하는데'),\n",
       " (1994, 2008, '요즘에는'),\n",
       " (2009, 2023, '인터넷으로'),\n",
       " (2024, 2038, '모든'),\n",
       " (2039, 2053, '걸'),\n",
       " (2054, 2068, '할'),\n",
       " (2069, 2083, '수'),\n",
       " (2084, 2098, '있어서'),\n",
       " (2099, 2113, '참'),\n",
       " (2114, 2128, '편해.'),\n",
       " (2165, 2181, '분명'),\n",
       " (2182, 2198, '나도'),\n",
       " (2199, 2215, '회사에'),\n",
       " (2216, 2232, '다니는데'),\n",
       " (2233, 2249, '집안일을'),\n",
       " (2250, 2266, '나'),\n",
       " (2267, 2283, '혼자서만'),\n",
       " (2284, 2299, '해.'),\n",
       " (2356, 2374, '언어'),\n",
       " (2375, 2393, '공부를'),\n",
       " (2394, 2412, '좋아해서'),\n",
       " (2413, 2431, '이번엔'),\n",
       " (2432, 2450, '중국어를'),\n",
       " (2451, 2469, '배워볼까'),\n",
       " (2470, 2488, '하는데'),\n",
       " (2489, 2507, '많이'),\n",
       " (2508, 2526, '어려울까'),\n",
       " (2527, 2545, '봐'),\n",
       " (2546, 2564, '걱정돼.'),\n",
       " (2620, 2633, '오프라인으로'),\n",
       " (2634, 2647, '옷을'),\n",
       " (2648, 2661, '사면'),\n",
       " (2662, 2675, '내'),\n",
       " (2676, 2689, '몸에'),\n",
       " (2690, 2702, '딱'),\n",
       " (2703, 2715, '맞는'),\n",
       " (2716, 2728, '옷을'),\n",
       " (2729, 2741, '살'),\n",
       " (2742, 2754, '수'),\n",
       " (2755, 2767, '있어서'),\n",
       " (2768, 2780, '좋아요.'),\n",
       " (2838, 2849, '할'),\n",
       " (2850, 2861, '일을'),\n",
       " (2862, 2873, '하기'),\n",
       " (2874, 2885, '전에'),\n",
       " (2886, 2897, '책상'),\n",
       " (2898, 2908, '정리를'),\n",
       " (2909, 2919, '하는'),\n",
       " (2920, 2930, '걸'),\n",
       " (2931, 2941, '좋아해.'),\n",
       " (2976, 2995, '취미용으로'),\n",
       " (2996, 3014, '첼로를'),\n",
       " (3015, 3033, '사려고'),\n",
       " (3034, 3052, '하는데'),\n",
       " (3053, 3071, '어느'),\n",
       " (3072, 3090, '정도'),\n",
       " (3091, 3109, '가격대가'),\n",
       " (3110, 3128, '좋을까?'),\n",
       " (3193, 3206, '오늘도'),\n",
       " (3207, 3220, '퇴근하고'),\n",
       " (3221, 3234, '오는'),\n",
       " (3235, 3248, '길에'),\n",
       " (3249, 3262, '애들'),\n",
       " (3263, 3276, '먹을'),\n",
       " (3277, 3289, '치킨'),\n",
       " (3290, 3302, '한'),\n",
       " (3303, 3315, '마리'),\n",
       " (3316, 3328, '사서'),\n",
       " (3329, 3341, '가는'),\n",
       " (3342, 3354, '중이야.'),\n",
       " (3415, 3428, '오늘은'),\n",
       " (3429, 3442, '오랜만에'),\n",
       " (3443, 3455, '씻어서'),\n",
       " (3456, 3468, '그런지'),\n",
       " (3469, 3481, '샴푸를'),\n",
       " (3482, 3494, '두'),\n",
       " (3495, 3507, '번'),\n",
       " (3508, 3520, '해도'),\n",
       " (3521, 3533, '거품이'),\n",
       " (3534, 3546, '잘'),\n",
       " (3547, 3559, '안'),\n",
       " (3560, 3572, '나네?'),\n",
       " (3643, 3664, '긴장하면'),\n",
       " (3665, 3686, '손톱을'),\n",
       " (3687, 3708, '물어뜯는'),\n",
       " (3709, 3730, '버릇이'),\n",
       " (3731, 3752, '있어.'),\n",
       " (3753, 3773, '중요한'),\n",
       " (3774, 3794, '시험이라도'),\n",
       " (3795, 3815, '보는'),\n",
       " (3816, 3836, '날에는'),\n",
       " (3837, 3857, '내'),\n",
       " (3858, 3878, '손톱이'),\n",
       " (3879, 3899, '남아나질'),\n",
       " (3900, 3920, '않아.'),\n",
       " (3956, 3975, '어제'),\n",
       " (3976, 3995, '술'),\n",
       " (3996, 4015, '마시다가'),\n",
       " (4016, 4034, '핸드폰을'),\n",
       " (4035, 4053, '깨트려서'),\n",
       " (4054, 4072, '오늘'),\n",
       " (4073, 4091, '핸드폰'),\n",
       " (4092, 4110, '수리하러'),\n",
       " (4111, 4129, '가야'),\n",
       " (4130, 4148, '해.'),\n",
       " (4213, 4230, '머리를'),\n",
       " (4231, 4247, '감고'),\n",
       " (4248, 4264, '머리를'),\n",
       " (4265, 4281, '드라이기로'),\n",
       " (4282, 4298, '말리면'),\n",
       " (4299, 4315, '바로'),\n",
       " (4316, 4332, '땀이'),\n",
       " (4333, 4349, '나서'),\n",
       " (4350, 4366, '찝찝해.'),\n",
       " (4435, 4450, '우리'),\n",
       " (4451, 4466, '집에서'),\n",
       " (4467, 4482, '자주'),\n",
       " (4483, 4498, '같이'),\n",
       " (4499, 4514, '보는'),\n",
       " (4515, 4530, '보드게임을'),\n",
       " (4531, 4546, '하는'),\n",
       " (4547, 4561, '친구들이'),\n",
       " (4562, 4576, '오늘도'),\n",
       " (4577, 4591, '놀러'),\n",
       " (4592, 4606, '왔어.'),\n",
       " (4671, 4686, '밖에'),\n",
       " (4687, 4702, '날씨가'),\n",
       " (4703, 4718, '추워서'),\n",
       " (4719, 4734, '옷을'),\n",
       " (4735, 4750, '좀'),\n",
       " (4751, 4765, '더'),\n",
       " (4766, 4780, '껴입어야겠어.'),\n",
       " (4840, 4857, '머리가'),\n",
       " (4858, 4875, '너무'),\n",
       " (4876, 4893, '길어서'),\n",
       " (4894, 4911, '집에'),\n",
       " (4912, 4929, '오는'),\n",
       " (4930, 4947, '길에'),\n",
       " (4948, 4964, '이발소에'),\n",
       " (4965, 4981, '들르려고.'),\n",
       " (5017, 5029, '지난주에'),\n",
       " (5030, 5042, '봤던'),\n",
       " (5043, 5055, '시험'),\n",
       " (5056, 5068, '결과가'),\n",
       " (5069, 5081, '오늘'),\n",
       " (5082, 5094, '나와서'),\n",
       " (5095, 5107, '기분이'),\n",
       " (5108, 5119, '안'),\n",
       " (5120, 5131, '좋아.'),\n",
       " (5209, 5223, '무언가를'),\n",
       " (5224, 5238, '유심히'),\n",
       " (5239, 5253, '살펴볼'),\n",
       " (5254, 5268, '때'),\n",
       " (5269, 5283, '인상을'),\n",
       " (5284, 5297, '쓰는'),\n",
       " (5298, 5311, '것은'),\n",
       " (5312, 5325, '내'),\n",
       " (5326, 5339, '버릇이야.'),\n",
       " (5409, 5422, '친구가'),\n",
       " (5423, 5436, '집에'),\n",
       " (5437, 5450, '놀러'),\n",
       " (5451, 5464, '오기로'),\n",
       " (5465, 5478, '했는데'),\n",
       " (5479, 5491, '아직'),\n",
       " (5492, 5504, '청소를'),\n",
       " (5505, 5517, '못'),\n",
       " (5518, 5530, '했어.'),\n",
       " (5581, 5601, '애들'),\n",
       " (5602, 5622, '방학이'),\n",
       " (5623, 5643, '끝나서'),\n",
       " (5644, 5664, '드디어'),\n",
       " (5665, 5685, '집이'),\n",
       " (5686, 5706, '조금'),\n",
       " (5707, 5726, '조용해졌네요.'),\n",
       " (5807, 5821, '다음'),\n",
       " (5822, 5836, '주에'),\n",
       " (5837, 5850, '피부과에'),\n",
       " (5851, 5864, '가서'),\n",
       " (5865, 5878, '점을'),\n",
       " (5879, 5892, '빼려고.'),\n",
       " (5965, 5978, '오늘도'),\n",
       " (5979, 5992, '퇴근하고'),\n",
       " (5993, 6006, '학원'),\n",
       " (6007, 6020, '끝난'),\n",
       " (6021, 6034, '딸을'),\n",
       " (6035, 6048, '데리고'),\n",
       " (6049, 6061, '집에'),\n",
       " (6062, 6074, '가.'),\n",
       " (6142, 6161, '이번'),\n",
       " (6162, 6181, '주말은'),\n",
       " (6182, 6201, '쇼파에서'),\n",
       " (6202, 6220, '커피'),\n",
       " (6221, 6239, '한'),\n",
       " (6240, 6258, '잔'),\n",
       " (6259, 6277, '마시면서'),\n",
       " (6278, 6296, '조용한'),\n",
       " (6297, 6315, '영화'),\n",
       " (6316, 6334, '한'),\n",
       " (6335, 6353, '편'),\n",
       " (6354, 6372, '틀어'),\n",
       " (6373, 6391, '놓고'),\n",
       " (6392, 6410, '뜨개질하면서'),\n",
       " (6411, 6429, '보내려고.'),\n",
       " (6497, 6516, '취미로'),\n",
       " (6517, 6536, '여러'),\n",
       " (6537, 6556, '자격증을'),\n",
       " (6557, 6576, '취득했는데'),\n",
       " (6577, 6595, '너무'),\n",
       " (6596, 6614, '뿌듯하고'),\n",
       " (6615, 6633, '재밌어.'),\n",
       " (6699, 6713, '오늘은'),\n",
       " (6714, 6728, '비가'),\n",
       " (6729, 6743, '온다고'),\n",
       " (6744, 6758, '해서'),\n",
       " (6759, 6773, '우산을'),\n",
       " (6774, 6788, '챙겨'),\n",
       " (6789, 6802, '나가야겠어.'),\n",
       " (6877, 6889, '오늘도'),\n",
       " (6890, 6902, '퇴근하고'),\n",
       " (6903, 6915, '오는'),\n",
       " (6916, 6928, '길에'),\n",
       " (6929, 6941, '애들'),\n",
       " (6942, 6954, '먹을'),\n",
       " (6955, 6967, '치킨'),\n",
       " (6968, 6980, '한'),\n",
       " (6981, 6992, '마리'),\n",
       " (6993, 7004, '사서'),\n",
       " (7005, 7016, '가는'),\n",
       " (7017, 7028, '중이야.'),\n",
       " (7097, 7115, '오랜만에'),\n",
       " (7116, 7134, '소개팅을'),\n",
       " (7135, 7153, '나갈'),\n",
       " (7154, 7172, '건데'),\n",
       " (7173, 7190, '입을'),\n",
       " (7191, 7208, '옷이'),\n",
       " (7209, 7226, '없어서'),\n",
       " (7227, 7244, '쇼핑해야겠어.'),\n",
       " (7300, 7320, '지난주에'),\n",
       " (7321, 7341, '쇼핑하려고'),\n",
       " (7342, 7362, '백화점에'),\n",
       " (7363, 7383, '갔었는데'),\n",
       " (7384, 7404, '사람이'),\n",
       " (7405, 7424, '얼마나'),\n",
       " (7425, 7444, '많은지'),\n",
       " (7445, 7464, '깜짝'),\n",
       " (7465, 7484, '놀랐어요.'),\n",
       " (7561, 7581, '모기'),\n",
       " (7582, 7602, '때문에'),\n",
       " (7603, 7623, '요즘'),\n",
       " (7624, 7644, '스트레스야.'),\n",
       " (7645, 7665, '모기'),\n",
       " (7666, 7686, '퇴치하는'),\n",
       " (7687, 7707, '방법'),\n",
       " (7708, 7728, '좀'),\n",
       " (7729, 7748, '알려줘.'),\n",
       " (7823, 7841, '작년'),\n",
       " (7842, 7860, '여름옷이'),\n",
       " (7861, 7879, '다'),\n",
       " (7880, 7898, '작아져서'),\n",
       " (7899, 7917, '쇼핑했더니'),\n",
       " (7918, 7936, '돈을'),\n",
       " (7937, 7954, '엄청'),\n",
       " (7955, 7972, '많이'),\n",
       " (7973, 7990, '썼어요.'),\n",
       " (8072, 8089, '오늘'),\n",
       " (8090, 8107, '기온이'),\n",
       " (8108, 8125, '삼십'),\n",
       " (8126, 8142, '도로'),\n",
       " (8143, 8159, '엄청'),\n",
       " (8160, 8176, '후덥지근했어.'),\n",
       " (8251, 8262, '자기'),\n",
       " (8263, 8274, '관리는'),\n",
       " (8275, 8286, '꾸준히'),\n",
       " (8287, 8298, '하는'),\n",
       " (8299, 8310, '게'),\n",
       " (8311, 8322, '중요해.'),\n",
       " (8380, 8394, '쇼핑'),\n",
       " (8395, 8409, '말고'),\n",
       " (8410, 8424, '다른'),\n",
       " (8425, 8439, '취미가'),\n",
       " (8440, 8453, '필요해.'),\n",
       " (8520, 8538, '오늘'),\n",
       " (8539, 8556, '어디'),\n",
       " (8557, 8574, '가실래?'),\n",
       " (8575, 8592, '그렇게'),\n",
       " (8593, 8610, '차려입으셨어요?'),\n",
       " (8648, 8661, '저는'),\n",
       " (8662, 8675, '할'),\n",
       " (8676, 8689, '일을'),\n",
       " (8690, 8703, '하면서'),\n",
       " (8704, 8717, '주변을'),\n",
       " (8718, 8731, '정리하는'),\n",
       " (8732, 8745, '습관이'),\n",
       " (8746, 8758, '있어요.'),\n",
       " (8844, 8856, '뭘'),\n",
       " (8857, 8869, '사야'),\n",
       " (8870, 8882, '할지'),\n",
       " (8883, 8895, '미리'),\n",
       " (8896, 8907, '정해두고'),\n",
       " (8908, 8919, '쇼핑하는'),\n",
       " (8920, 8931, '게'),\n",
       " (8932, 8943, '좋을'),\n",
       " (8944, 8955, '것'),\n",
       " (8956, 8967, '같아.'),\n",
       " (9042, 9057, '나는'),\n",
       " (9058, 9073, '꿈을'),\n",
       " (9074, 9088, '많이'),\n",
       " (9089, 9103, '꾸는'),\n",
       " (9104, 9118, '편인데'),\n",
       " (9119, 9133, '어젯밤'),\n",
       " (9134, 9148, '꿈도'),\n",
       " (9149, 9163, '생생하게'),\n",
       " (9164, 9178, '기억나.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps = 30\n",
    "num_frames = len(frame_paths[0])\n",
    "\n",
    "frame_text_mapping = map_frames_to_text(frame_paths[0], annotations[0], fps, num_frames, max_sequence_length=30)\n",
    "frame_text_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0903a93-8e43-4841-97e5-9109fb6c9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_video_frame(frame_path, transform):\n",
    "    img = Image.open(frame_path).convert('RGB')\n",
    "    print(f\"Loaded frame: {frame_path}, Size: {img.size}\")  # 프레임 경로와 크기 출력\n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True)\n",
    "    return {\n",
    "        'input_ids': tokens['input_ids'],  # Actual tokenized input\n",
    "        'attention_mask': tokens['attention_mask']  # Actual attention mask\n",
    "    }\n",
    "\n",
    "class LipReadingDataset(Dataset):\n",
    "    def __init__(self, video_paths, annotations, tokenizer, transform, fps, sequence_length=30):\n",
    "        self.video_paths = video_paths\n",
    "        self.annotations = annotations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length  # Set to 30 frames\n",
    "        self.fps = fps\n",
    "\n",
    "        # Frame-text mapping creation using map_frames_to_text function\n",
    "        self.frame_text_mapping = []\n",
    "        for video_annotation in annotations:\n",
    "            num_frames = len(video_paths[0])  # Number of frames in each video\n",
    "            mapping = map_frames_to_text(video_paths[0], video_annotation, fps, num_frames, sequence_length)\n",
    "            self.frame_text_mapping.append(mapping)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(mapping) for mapping in self.frame_text_mapping])  # Total number of 30-frame chunks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find the video and frame index based on idx\n",
    "        video_idx, frame_mapping_idx = self.get_video_frame_index(idx)\n",
    "\n",
    "        # Get the frame-text mapping for this sequence\n",
    "        start_frame, end_frame, text = self.frame_text_mapping[video_idx][frame_mapping_idx]\n",
    "\n",
    "        # Load the frames for this sequence\n",
    "        frames = []\n",
    "        for frame_idx in range(start_frame, min(end_frame + 1, len(self.video_paths[video_idx]))):\n",
    "            frame_path = self.video_paths[video_idx][frame_idx]\n",
    "            frame = preprocess_video_frame(frame_path, self.transform)\n",
    "            frames.append(frame)\n",
    "\n",
    "        # Padding for remaining frames if sequence is shorter than 30 frames\n",
    "        while len(frames) < self.sequence_length:\n",
    "            frames.append(torch.zeros_like(frames[0]))  # Add zero padding for frames\n",
    "\n",
    "        # Combine and tokenize the text\n",
    "        tokens = tokenize_text(text, self.tokenizer)\n",
    "\n",
    "        return {\n",
    "            'video': torch.stack(frames),  # (sequence_length, C, H, W) - 30 frames\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),  # (max_length,)\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def get_video_frame_index(self, idx):\n",
    "        \"\"\"Find the video and the 30-frame chunk index\"\"\"\n",
    "        cum_frames = 0\n",
    "        for i, mapping in enumerate(self.frame_text_mapping):\n",
    "            if idx < cum_frames + len(mapping):\n",
    "                return i, idx - cum_frames\n",
    "            cum_frames += len(mapping)\n",
    "        raise IndexError(\"Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccf7782e-489b-44ba-8968-c96cecb1291d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0117.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0118.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0119.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0120.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0121.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0122.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0123.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0124.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0125.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0126.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0127.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0128.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0129.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0130.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0131.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 1250, 7715,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0132.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0133.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0134.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0135.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0136.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0137.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0138.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0139.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0140.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0141.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0142.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0143.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0144.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0145.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([   2, 1851,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0146.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0147.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0148.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0149.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0150.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0151.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0152.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0153.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0154.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0155.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0156.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0157.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0158.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0159.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([   2, 4999, 6968,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0160.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0161.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0162.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0163.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0164.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0165.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0166.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0167.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0168.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0169.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0170.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0171.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0172.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0173.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([   2, 4324, 5210,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0174.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0175.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0176.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0177.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0178.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0179.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0180.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0181.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0182.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0183.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0184.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0185.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0186.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0187.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([   2, 3106,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0188.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0189.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0190.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0191.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0192.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0193.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0194.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0195.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0196.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0197.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0198.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0199.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0200.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0201.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([  2, 785,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0202.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0203.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0204.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0205.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0206.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0207.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0208.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0209.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0210.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0211.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0212.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0213.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0214.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0215.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([14, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0249.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0250.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0251.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0252.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0253.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0254.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0255.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0256.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0257.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0258.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0259.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0260.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0261.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0262.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0263.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0264.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([16, 3, 224, 224]), Input IDs: tensor([   2, 1457,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0265.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0266.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0267.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0268.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0269.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0270.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0271.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0272.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0273.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0274.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0275.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0276.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0277.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0278.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0279.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0280.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([16, 3, 224, 224]), Input IDs: tensor([   2, 4093, 6776,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0281.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0282.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0283.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0284.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0285.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0286.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0287.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0288.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0289.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0290.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0291.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0292.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0293.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0294.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0295.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 4507,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0296.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0297.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0298.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0299.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0300.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0301.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0302.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0303.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0304.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0305.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0306.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0307.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0308.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0309.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0310.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 2584, 7006, 6991,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0311.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0312.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0313.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0314.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0315.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0316.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0317.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0318.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0319.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0320.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0321.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0322.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0323.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0324.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0325.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 3299,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0326.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0327.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0328.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0329.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0330.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0331.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0332.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0333.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0334.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0335.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0336.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0337.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0338.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0339.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0340.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 1447,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0341.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0342.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0343.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0344.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0345.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0346.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0347.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0348.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0349.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0350.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0351.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0352.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0353.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0354.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0355.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 4508, 5348,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0356.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0357.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0358.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0359.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0360.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0361.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0362.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0363.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0364.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0365.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0366.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0367.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0368.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0369.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0370.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 2584, 7006,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0371.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0372.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0373.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0374.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0375.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0376.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0377.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0378.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0379.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0380.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0381.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0382.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0383.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0384.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0385.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 2620, 6968,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0386.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0387.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0388.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0389.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0390.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0391.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0392.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0393.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0394.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0395.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0396.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0397.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0398.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0399.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0400.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([   2, 2453, 5917,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0401.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0402.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0403.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0404.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0405.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0406.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0407.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0408.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0409.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0410.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0411.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0412.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0413.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0414.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0415.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([  2, 661,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0416.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0417.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0418.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0419.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0420.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0421.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0422.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0423.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0424.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0425.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0426.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0427.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0428.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0429.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0430.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([15, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0629.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0630.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0631.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0632.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0633.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0634.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0635.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0636.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0637.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([9, 3, 224, 224]), Input IDs: tensor([   2, 3015,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0638.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0639.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0640.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0641.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0642.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0643.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0644.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0645.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0646.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([9, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0749.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0750.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0751.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0752.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0753.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0754.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0755.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0756.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0757.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0758.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0759.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0760.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0761.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0762.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0763.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0764.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0765.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0766.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0767.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3813,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0768.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0769.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0770.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0771.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0772.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0773.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0774.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0775.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0776.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0777.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0778.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0779.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0780.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0781.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0782.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0783.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0784.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0785.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0786.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0787.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0788.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0789.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0790.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0791.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0792.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0793.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0794.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0795.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0796.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0797.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0798.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0799.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0800.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0801.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0802.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0803.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0804.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0805.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3381,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0806.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0807.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0808.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0809.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0810.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0811.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0812.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0813.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0814.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0815.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0816.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0817.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0818.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0819.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0820.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0821.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0822.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0823.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0824.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3001, 5902,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0825.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0826.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0827.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0828.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0829.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0830.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0831.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0832.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0833.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0834.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0835.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0836.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0837.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0838.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0839.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0840.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0841.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0842.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0843.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 4863,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0844.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0845.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0846.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0847.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0848.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0849.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0850.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0851.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0852.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0853.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0854.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0855.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0856.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0857.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0858.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0859.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0860.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0861.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0862.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2,  903, 6902,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0863.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0864.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0865.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0866.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0867.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0868.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0869.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0870.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0871.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0872.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0873.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0874.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0875.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0876.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0877.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0878.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0879.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0880.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0881.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 1284, 6822,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0882.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0883.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0884.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0885.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0886.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0887.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0888.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0889.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0890.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0891.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0892.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0893.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0894.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0895.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0896.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0897.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0898.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0899.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0900.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([  2, 648,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0901.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0902.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0903.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0904.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0905.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0906.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0907.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0908.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0909.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0910.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0911.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0912.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0913.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0914.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0915.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0916.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0917.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0918.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0919.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0920.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0921.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0922.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0923.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0924.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0925.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0926.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0927.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0928.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0929.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0930.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0931.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0932.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0933.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0934.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0935.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0936.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0937.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0938.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 2581, 6022,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0939.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0940.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0941.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0942.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0943.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0944.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0945.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0946.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0947.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0948.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0949.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0950.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0951.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0952.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0953.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0954.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0955.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0956.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0957.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 1948, 6976,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0958.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0959.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0960.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0961.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0962.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0963.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0964.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0965.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0966.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0967.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0968.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0969.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0970.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0971.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0972.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0973.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0974.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0975.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0976.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 1838, 5883, 6433,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0977.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0978.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0979.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0980.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0981.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0982.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0983.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0984.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0985.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0986.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0987.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0988.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0989.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0990.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0991.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0992.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0993.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0994.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0995.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3825, 6783,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0996.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0997.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0998.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_0999.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1000.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1001.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1002.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1003.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1004.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1005.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1006.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1007.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1008.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1009.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1010.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1011.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1012.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1013.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1014.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 1222, 5641,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1015.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1016.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1017.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1018.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1019.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1020.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1021.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1022.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1023.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1024.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1025.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1026.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1027.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1028.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1029.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1030.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1031.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1032.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1033.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3622, 5640,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1034.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1035.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1036.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1037.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1038.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1039.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1040.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1041.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1042.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1043.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1044.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1045.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1046.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1047.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1048.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1049.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1050.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1051.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1052.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 4510, 5688,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1053.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1054.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1055.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1056.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1057.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1058.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1059.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1060.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1061.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1062.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1063.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1064.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1065.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1066.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1067.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1068.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1069.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1070.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1071.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 3275, 6776,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1072.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1073.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1074.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1075.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1076.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1077.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1078.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1079.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1080.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1081.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1082.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1083.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1084.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1085.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1086.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1087.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1088.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1089.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1090.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 2015, 6968,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1091.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1092.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1093.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1094.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1095.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1096.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1097.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1098.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1099.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1100.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1101.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1102.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1103.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1104.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1105.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1106.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1107.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1108.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1109.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([   2, 1654,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1110.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1111.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1112.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1113.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1114.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1115.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1116.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1117.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1118.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1119.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1120.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1121.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1122.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1123.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1124.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1125.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1126.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1127.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([18, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1427.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1428.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1429.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1430.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1431.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1432.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1433.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1434.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1435.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1436.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1437.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1438.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1439.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1440.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1441.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1442.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1443.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1444.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1445.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1446.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1447.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1448.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([22, 3, 224, 224]), Input IDs: tensor([   2, 3369,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1449.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1450.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1451.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1452.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1453.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1454.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1455.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1456.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1457.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1458.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1459.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1460.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1461.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1462.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1463.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1464.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1465.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1466.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1467.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1468.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1469.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1470.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([22, 3, 224, 224]), Input IDs: tensor([   2, 4878, 5210,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1471.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1472.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1473.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1474.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1475.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1476.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1477.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1478.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1479.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1480.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1481.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1482.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1483.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1484.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1485.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1486.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1487.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1488.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1489.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1490.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1491.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([21, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1492.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1493.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1494.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1495.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1496.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1497.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1498.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1499.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1500.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1501.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1502.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1503.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1504.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1505.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1506.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1507.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1508.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1509.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1510.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1511.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1512.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([21, 3, 224, 224]), Input IDs: tensor([   2, 4640, 5426, 7691,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1513.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1514.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1515.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1516.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1517.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1518.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1519.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1520.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1521.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1522.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1523.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1524.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1525.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1526.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1527.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1528.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1529.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1530.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1531.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1532.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1533.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([21, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1565.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1566.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1567.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1568.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1569.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1570.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1571.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1572.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1573.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1574.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1575.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1576.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1577.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1578.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1579.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1580.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1581.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1582.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1583.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1584.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([20, 3, 224, 224]), Input IDs: tensor([   2, 2844,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1585.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1586.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1587.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1588.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1589.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1590.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1591.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1592.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1593.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1594.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1595.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1596.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1597.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1598.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1599.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1600.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1601.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1602.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1603.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1604.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([20, 3, 224, 224]), Input IDs: tensor([   2, 1250, 6078,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1605.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1606.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1607.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1608.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1609.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1610.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1611.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1612.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1613.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1614.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1615.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1616.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1617.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1618.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1619.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1620.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1621.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1622.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1623.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1624.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([20, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1625.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1626.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1627.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1628.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1629.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1630.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1631.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1632.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1633.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1634.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1635.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1636.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1637.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1638.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1639.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1640.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1641.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1642.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1643.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1644.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([20, 3, 224, 224]), Input IDs: tensor([   2, 2252, 6433,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1645.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1646.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1647.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1648.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1649.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1650.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1651.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1652.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1653.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1654.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1655.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1656.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1657.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1658.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1659.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1660.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1661.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1662.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1663.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1664.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1665.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1666.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1667.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1668.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1669.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1670.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1671.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1672.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1673.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1674.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1675.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1676.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1677.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1678.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1679.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1680.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1681.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1682.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([19, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1720.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1721.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1722.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1723.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1724.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1725.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1726.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1727.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1728.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1729.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1730.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1731.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1732.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1733.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1734.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1735.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1736.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1737.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([18, 3, 224, 224]), Input IDs: tensor([   2, 2578,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1738.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1739.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1740.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1741.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1742.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1743.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1744.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1745.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1746.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1747.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1748.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1749.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1750.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1751.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1752.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1753.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1754.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([   2, 2520,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1755.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1756.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1757.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1758.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1759.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1760.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1761.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1762.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1763.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1764.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1765.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1766.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1767.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1768.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1769.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1770.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1771.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1772.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1773.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1774.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1775.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1776.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1777.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1778.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1779.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1780.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1781.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1782.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1783.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1784.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1785.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1786.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1787.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1788.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([   2, 2498, 7190, 5210,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1789.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1790.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1791.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1792.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1793.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1794.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1795.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1796.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1797.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1798.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1799.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1800.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1801.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1802.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1803.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1804.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1805.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([   2, 3015,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1806.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1807.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1808.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1809.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1810.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1811.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1812.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1813.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1814.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1815.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1816.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1817.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1818.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1819.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1820.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1821.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1822.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([   2, 1854, 6433,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1823.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1824.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1825.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1826.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1827.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1828.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1829.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1830.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1831.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1832.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1833.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1834.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1835.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1836.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1837.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1838.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1839.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([   2,  994, 7662, 5917,    3,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1840.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1841.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1842.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1843.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1844.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1845.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1846.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1847.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1848.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1849.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1850.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1851.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1852.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1853.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1854.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1855.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1856.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([17, 3, 224, 224]), Input IDs: tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1930.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1931.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1932.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1933.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1934.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1935.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1936.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1937.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1938.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1939.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1940.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1941.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1942.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1943.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1944.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1945.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([16, 3, 224, 224]), Input IDs: tensor([   2, 4264,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1946.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1947.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1948.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1949.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1950.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1951.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1952.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1953.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1954.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1955.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1956.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1957.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1958.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1959.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1960.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1961.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([16, 3, 224, 224]), Input IDs: tensor([   2, 2085,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1962.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1963.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1964.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1965.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1966.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1967.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1968.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1969.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1970.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1971.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1972.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1973.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1974.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1975.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1976.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1977.jpg, Size: (1920, 1080)\n",
      "Video: torch.Size([16, 3, 224, 224]), Input IDs: tensor([   2, 1251, 5441,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1]), Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1978.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1979.jpg, Size: (1920, 1080)\n",
      "Loaded frame: /root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/frame_1980.jpg, Size: (1920, 1080)\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LipReadingDataset(frame_paths, annotations, tokenizer, transform, fps, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Input IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Attention Mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 47\u001b[0m, in \u001b[0;36mLipReadingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_frame, \u001b[38;5;28mmin\u001b[39m(end_frame \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_paths[video_idx]))):\n\u001b[1;32m     46\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_paths[video_idx][frame_idx]\n\u001b[0;32m---> 47\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_video_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Padding for remaining frames if sequence is shorter than 30 frames\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m, in \u001b[0;36mpreprocess_video_frame\u001b[0;34m(frame_path, transform)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_video_frame\u001b[39m(frame_path, transform):\n\u001b[0;32m----> 4\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded frame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 프레임 경로와 크기 출력\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m transform(img)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/root/lip-to-speech/split_TS163/lip_K_5_F_06_C651_A_001.mp4/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"beomi/kobert\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "fps = 30\n",
    "\n",
    "dataset = LipReadingDataset(frame_paths, annotations, tokenizer, transform, fps, sequence_length=5)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    data = dataset[i]\n",
    "    print(f\"Video: {data['video'].shape}, Input IDs: {data['input_ids']}, Attention Mask: {data['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6d28b-bb29-4d8f-9b8b-f19283f7249c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66248136-3c79-4085-9086-a5d32a9d247b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
